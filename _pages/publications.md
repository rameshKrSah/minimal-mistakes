---
title: "Publications"
permalink: /publications/
header:
comments: false
---

## Conference Papers
- **Ramesh Kumar Sah** and Hassan Ghasemzadeh, **Adar: Adversarial Activity Recognition in Wearables**, *The 38th IEEE/ACM International Conference On Computer Aided Design (ICCAD)*, November 4-7, 2019, Westminister, CO. [Link](https://www.semanticscholar.org/paper/Adar%3A-Adversarial-Activity-Recognition-in-Wearables-Sah-Ghasemzadeh/2e5222171b5eb8c9dd855d7bf96bf54e661ea9a1)

   Abstract: Recent advances in machine learning and deep neural networks have led to the realization of many important applications in the area of personalized medicine. Whether it is detecting activities of daily living or analyzing images for cancerous cells, machine learning algorithms have become the dominant choice for such emerging applications. In particular, the state-of-the-art algorithms used for human activity recognition (HAR) using wearable inertial sensors utilize machine learning algorithms to detect health events and to make predictions from sensor data. Currently, however, there remains a gap in research on whether or not and how activity recognition algorithms may become the subject of adversarial attacks. In this paper, we take the first strides on (1) investigating methods of generating adversarial example in the context of HAR systems; (2) studying the vulnerability of activity recognition models to adversarial examples in feature and signal domain; and (3) investigating the effects of adversarial training on HAR systems. We introduce Adar11Software code and experimental data for Adar are available online at https://github.com/rameshKrSah/Adar., a novel computational framework for optimization-driven creation of adversarial examples in sensor-based activity recognition systems. Through extensive analysis based on real sensor data collected with human subjects, we found that simple evasion attacks are able to decrease the accuracy of a deep neural network from 95.1% to 3.4% and from 93.1% to 16.8% in the case of a convolutional neural network. With adversarial training, the robustness of the deep neural network increased on the adversarial examples by 49.1% in the worst case while the accuracy on clean samples decreased by 13.2%

***

- **Ramesh Kumar Sah**, Hassan Ghasemzadeh, Assal Habibi, Michael McDonell, Patricia Pendry, and Michael Cleveland, **Mobile Health for Alcohol Recovery and Relapse Prevention**, *2020 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)*, December16-18, 2020. [IEEE Link](https://ieeexplore.ieee.org/abstract/document/9327931). [PDF](../assets/files/stress_poster.pdf)

   Abstract: Alcohol related disorder has increasingly become a serious public health issue. Stress detection and intervention is considered a key element in a treatment strategy towards preventing alcohol dependent individuals from relapsing. In this paper, we present a proof-of-concept approach to study the usability of a wearable device and viability of a mobile health application to prevent alcohol relapse by detecting moments of stress and providing adaptive interventions in real-time.

***
***
## PrePrint 
+ **Ramesh Kumar Sah** and Hassan Ghasemzadeh, **Adversarial Transferability in Wearable Systems**, March-17, 2020. [Arxiv-PrePrint](https://arxiv.org/abs/2003.07982)

   Abstract: Machine learning has increasingly become the most used approach for inference and decision making in wearable sensor systems. However, recent studies have found that machine learning systems are easily fooled by the addition of adversarial perturbation to their inputs. What is more interesting is that the adversarial examples generated for one machine learning system can also degrade the performance of another. This property of adversarial examples is called transferability. In this work, we take the first strides in studying adversarial transferability in wearable sensor systems, from the following perspectives: 1) Transferability between machine learning models, 2) Transferability across subjects, 3) Transferability across sensor locations, and 4) Transferability across datasets. With Human Activity Recognition (HAR) as an example sensor system, we found strong untargeted transferability in all cases of transferability. Specifically, gradient-based attacks were able to achieve higher misclassification rates compared to non-gradient attacks. The misclassification rate of untargeted adversarial examples ranged from 20% to 98%. For targeted transferability between machine learning models, the success rate of adversarial examples was 100% for iterative attack methods. However, the success rate for other types of targeted transferability ranged from 20% to 0%. Our findings strongly suggest that adversarial transferability has serious consequences not only in sensor systems but also across the broad spectrum of ubiquitous computing.