---
title: 	"Adar: Adversarial Activity Recognition in Wearables"
date: 	2019-10-30
mathjax: True
tags: 	[machine_learning]
---

# Adar: Adversarial Activity Recognition in Wearables
This is the accompanying blog post for my paper "Adar: Adversarial Activity Recognition in Wearables", which was
accepted in the 2018 International Conference on Computer Aided Design (ICCAD). In this blog post I will highlight the objective
and motivation for this project and discuss the important results from the experiments. But before we dive in into the
technicalities of the paper, let's first understand the problem at hand.

Human activity recognition (HAR) is a major research area in the field of mobile and ubiquitous computing. In the
recent years we have seen a rise in the adoption of wearables devices that are used to monitor different health markers
such as activity level, diet intake, medicine adherence, etc. Almost all of these devices use one or more machine
learning algorithms to measure these bio-markers and provide information to the user. However, recent studies have found
that machine learning systems which perform very well on inference and prediction tasks are often very vulnerable to
adversarial perturbations. In particular even the addition of a small amount of carefully computed perturbations to the
clean samples degrade the performance of machine learning systems significantly. For example consider the image below,
which is taken from "*Explaining and Harnessing Adversarial Examples*" by **Goodfellow and et. al**. This image shows
that not only the addition of adversarial perturbation to the clean sample is able to fool the classifier but also that
the classifier is more confident in it's incorrect prediction.
![gibbon](../assets/images/gibbon.jpg "Panda Vs. Gibbon adversarial example")




